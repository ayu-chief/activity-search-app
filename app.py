# app.py
import time
import re
import unicodedata
from typing import List, Dict

import streamlit as st
import pandas as pd
import gspread
from gspread.exceptions import APIError
from google.oauth2.service_account import Credentials

from sentence_transformers import SentenceTransformer, util
from rank_bm25 import BM25Okapi
import numpy as np

# -----------------------------------------------------------------------------
# åŸºæœ¬è¨­å®š
# -----------------------------------------------------------------------------
st.set_page_config(page_title="æ´»å‹•ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¤œç´¢", layout="wide")

# èª­ã¿è¾¼ã¿çµæœã®æ§ãˆã‚è¡¨ç¤ºç”¨ãƒ­ã‚°ï¼ˆExpanderã«ã¾ã¨ã‚ã‚‹ï¼‰
if "OPENED_LOG" not in st.session_state:
    st.session_state.OPENED_LOG = []  # [(title, sid), ...]

# ä¸€è¦§ã®æ®µéšè¡¨ç¤ºç”¨ï¼ˆã•ã‚‰ã«è¡¨ç¤ºï¼‰
if "show_n" not in st.session_state:
    st.session_state.show_n = 15
if "last_query" not in st.session_state:
    st.session_state.last_query = ""

# -----------------------------------------------------------------------------
# Google Sheets æ¥ç¶š
# -----------------------------------------------------------------------------
SERVICE_ACCOUNT_INFO = st.secrets["google_service_account"]  # Secretsã«ä¿å­˜ã—ãŸJSON
SPREADSHEET_IDS = [
    "1GCenO3IlDFrSITj1r90G_Vz_11D66POc8ny9HMtdCcM",
    "1Rjkgc6whTpg4FKUNLVSdzFSya-_tg42Wg4e10p-MmmI",
    "1PFDBuFuqxC4OWMCPjErP8uYYRovE55t-0oWsXNMCMqc",
    "1p4utUR9of_uSQNpzwJpSXgKiPrNur5nSTgHZvrbwmuc",
    "1HULvSdUAdSNdXXhPshu4mfwraf-bNq6zakFRhKF4Yfg",
]
SCOPES = [
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/drive",
]
creds = Credentials.from_service_account_info(SERVICE_ACCOUNT_INFO, scopes=SCOPES)
gc = gspread.authorize(creds)

# ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼ˆåºƒã‚æ¨å¥¨ï¼‰
BASE_WAIT = 8.0   # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆé–“ã®å¾…æ©Ÿï¼ˆç§’ï¼‰
MAX_RETRY = 6     # 429æ™‚ã®æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°

# -----------------------------------------------------------------------------
# æ­£è¦åŒ–ãƒ˜ãƒ«ãƒ‘
# -----------------------------------------------------------------------------
def normalize(s: str) -> str:
    if s is None:
        return ""
    s = unicodedata.normalize("NFKC", str(s))
    s = re.sub(r"\s+", " ", s).strip()
    return s

# -----------------------------------------------------------------------------
# ã‚·ãƒ¼ãƒˆ1æš â†’ ãƒ¬ã‚³ãƒ¼ãƒ‰åŒ–ï¼ˆãƒ©ãƒ™ãƒ«å–ã‚Šå‡ºã—ï¼‰
# -----------------------------------------------------------------------------
LABELS = [
    "æ ¡èˆå", "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å", "ãƒ†ãƒ¼ãƒ", "å¯¾è±¡ç”Ÿå¾’", "å¯¾è±¡", "å‚åŠ äººæ•°",
    "æº–å‚™ç‰©", "å®Ÿæ–½æ–¹æ³•", "å­ä¾›ãŸã¡ã®åå¿œ", "å­ã©ã‚‚ãŸã¡ã®åå¿œ", "è‰¯ã‹ã£ãŸç‚¹", "æ”¹å–„ç‚¹",
]

def extract_value(values: List[List[str]], label: str) -> str:
    lab = normalize(label)
    for row in values:
        for j, cell in enumerate(row):
            if lab and lab in normalize(cell):
                right = row[j+1:] if j+1 < len(row) else []
                toks = [c for c in right if str(c).strip()]
                if toks:
                    return " ".join(normalize(c) for c in toks).strip()
    return ""

def parse_sheet(values: List[List[str]]) -> Dict[str, str]:
    rec = {}
    for lab in LABELS:
        rec[lab] = extract_value(values, lab)

    # å­ã©ã‚‚/å­ä¾› ã‚’çµ±ä¸€
    if not rec.get("å­ä¾›ãŸã¡ã®åå¿œ"):
        rec["å­ä¾›ãŸã¡ã®åå¿œ"] = rec.get("å­ã©ã‚‚ãŸã¡ã®åå¿œ", "")

    out = {
        "æ ¡èˆå": rec.get("æ ¡èˆå", ""),
        "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å": rec.get("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å", ""),
        "ãƒ†ãƒ¼ãƒ": rec.get("ãƒ†ãƒ¼ãƒ", ""),
        "å¯¾è±¡": rec.get("å¯¾è±¡ç”Ÿå¾’", "") or rec.get("å¯¾è±¡", ""),
        "å‚åŠ äººæ•°": rec.get("å‚åŠ äººæ•°", ""),
        "æº–å‚™ç‰©": rec.get("æº–å‚™ç‰©", ""),
        "å®Ÿæ–½æ–¹æ³•": rec.get("å®Ÿæ–½æ–¹æ³•", ""),
        "å­ä¾›ãŸã¡ã®åå¿œ": rec.get("å­ä¾›ãŸã¡ã®åå¿œ", ""),
        "è‰¯ã‹ã£ãŸç‚¹": rec.get("è‰¯ã‹ã£ãŸç‚¹", ""),
        "æ”¹å–„ç‚¹": rec.get("æ”¹å–„ç‚¹", ""),
    }

    # ä¸»è¦ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒç©ºãªã‚‰ä¿é™ºã¨ã—ã¦å…¨ã‚»ãƒ«é€£çµ
    if not any(out.values()):
        flat = " ".join([" ".join([str(c) for c in r if str(c).strip()]) for r in values])
        out["ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å"] = out["ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å"] or "(åç§°æœªè¨­å®š)"
        out["ãƒ†ãƒ¼ãƒ"] = flat[:200]

    return out

# -----------------------------------------------------------------------------
# 429å›é¿ï¼šmetadata(title,sheetId) â†’ values.batchGet
# -----------------------------------------------------------------------------
def _short_id(sid: str) -> str:
    return f"{sid[:6]}â€¦{sid[-4:]}" if len(sid) > 12 else sid

def open_sheet_by_id(sid: str):
    """é–‹ã‘ãŸã‚‰ãƒ­ã‚°ã«è¿½åŠ ï¼ˆç”»é¢ã«ã¯ãã®å ´ã§å‡ºã•ãªã„ï¼‰"""
    for attempt in range(MAX_RETRY):
        try:
            sh = gc.open_by_key(sid)
            st.session_state.OPENED_LOG.append((sh.title, sid))
            return sh
        except APIError as e:
            code = getattr(getattr(e, "response", None), "status_code", None)
            if code == 429 and attempt < MAX_RETRY - 1:
                wait = BASE_WAIT * (2 ** attempt)
                st.warning(f"â³ Rate limit: open_by_key (retry {attempt+1}/{MAX_RETRY}) in {wait:.1f}s")
                time.sleep(wait)
                continue
            st.session_state.OPENED_LOG.append((f"âŒ FAILED: {sid}", sid))
            return None
        except Exception:
            st.session_state.OPENED_LOG.append((f"âŒ FAILED: {sid}", sid))
            return None

@st.cache_data(show_spinner=True, ttl=6*60*60)
def load_all_data_v2() -> pd.DataFrame:
    """worksheets()ã¯ä½¿ã‚ãšã€ã‚¿ã‚¤ãƒˆãƒ«ï¼†sheetIdã‚’å–å¾—â†’values.batchGetã§ä¸€æ‹¬å–å¾—"""
    rows = []
    for sid in SPREADSHEET_IDS:
        sh = open_sheet_by_id(sid)
        if not sh:
            continue

        # 1) ã‚¿ã‚¤ãƒˆãƒ«ã¨ sheetId (gid) ã‚’è»½é‡å–å¾—
        meta = None
        for attempt in range(MAX_RETRY):
            try:
                meta = sh.fetch_sheet_metadata(
                    params={"fields": "sheets(properties(title,sheetId))"}
                )
                break
            except APIError as e:
                code = getattr(getattr(e, "response", None), "status_code", None)
                if code == 429 and attempt < MAX_RETRY - 1:
                    wait = BASE_WAIT * (2 ** attempt)
                    st.warning(f"â³ Rate limit: fetch_sheet_metadata (retry {attempt+1}/{MAX_RETRY}) in {wait:.1f}s")
                    time.sleep(wait)
                    continue
                meta = None
                break
            except Exception:
                meta = None
                break
        if not meta:
            continue

        sheets_props = [s["properties"] for s in meta.get("sheets", []) if "properties" in s]
        title_to_gid = {p.get("title"): p.get("sheetId") for p in sheets_props}

        titles = [p.get("title") for p in sheets_props]
        # åˆ—å¹…ã¯å¿…è¦ã«å¿œã˜ã¦ç‹­ã‚ã‚‹ï¼ˆA:N ãªã©ï¼‰ã€‚ç‹­ã„ã»ã©é€Ÿã„
        ranges = [f"'{t}'!A:Q" for t in titles]

        # 2) ä¸€æ‹¬å–å¾—ï¼ˆvalues.batchGetï¼‰
        time.sleep(BASE_WAIT)
        vals_resp = None
        for attempt in range(MAX_RETRY):
            try:
                vals_resp = sh.values_batch_get(ranges=ranges, params={"majorDimension": "ROWS"})
                break
            except APIError as e:
                code = getattr(getattr(e, "response", None), "status_code", None)
                if code == 429 and attempt < MAX_RETRY - 1:
                    wait = BASE_WAIT * (2 ** attempt)
                    st.warning(f"â³ Rate limit: values.batchGet (retry {attempt+1}/{MAX_RETRY}) in {wait:.1f}s")
                    time.sleep(wait)
                    continue
                vals_resp = None
                break
            except Exception:
                vals_resp = None
                break
        if not vals_resp:
            continue

        for vr in vals_resp.get("valueRanges", []):
            rng = vr.get("range", "")
            ws_title = rng.split("!")[0].strip("'")
            vals = vr.get("values", [])
            if not vals:
                continue

            rec = parse_sheet(vals)
            if not any(rec.values()):
                continue

            rec["ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ"] = sh.title
            rec["ãƒ•ã‚¡ã‚¤ãƒ«ID"] = sid
            rec["ã‚·ãƒ¼ãƒˆå"] = ws_title
            rec["ã‚·ãƒ¼ãƒˆGID"] = title_to_gid.get(ws_title)
            rec["æ¤œç´¢ç”¨ãƒ†ã‚­ã‚¹ãƒˆ"] = " ".join([
                rec.get("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å",""), rec.get("ãƒ†ãƒ¼ãƒ",""), rec.get("å¯¾è±¡",""),
                rec.get("æº–å‚™ç‰©",""), rec.get("å®Ÿæ–½æ–¹æ³•",""),
                rec.get("å­ä¾›ãŸã¡ã®åå¿œ",""), rec.get("è‰¯ã‹ã£ãŸç‚¹",""), rec.get("æ”¹å–„ç‚¹","")
            ]).strip()
            rows.append(rec)

    return pd.DataFrame(rows)

# -----------------------------------------------------------------------------
# UIï¼ˆãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿ï¼‰
# -----------------------------------------------------------------------------
st.title("æ´»å‹•ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¤œç´¢")

with st.sidebar:
    st.header("è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰")
    mode = st.radio(
        "ãƒšãƒ¼ã‚¸ã‚’é¸æŠ",
        ["ğŸ” æ¤œç´¢", "ğŸ“‘ ã‚·ãƒ¼ãƒˆåˆ¥ä¸€è¦§"],
        index=0,
        help="æ¤œç´¢ãƒšãƒ¼ã‚¸ã¨ã€ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ/ã‚·ãƒ¼ãƒˆã”ã¨ã®ä¸€è¦§ã‚’åˆ‡ã‚Šæ›¿ãˆã¾ã™ã€‚",
    )

    st.header("æ¤œç´¢è¨­å®š")
    alpha = st.slider("æ„å‘³é‡è¦–ï¼ˆ1.0ï¼‰ â†â†’ èªä¸€è‡´é‡è¦–ï¼ˆ0.0ï¼‰", 0.0, 1.0, 0.7, 0.05)
    # top_k ã¯ã€Œæœ€å¤§è¨ˆç®—ä»¶æ•°ã€ï¼ˆæ®µéšè¡¨ç¤ºã§ã“ã“ã¾ã§å‡ºã›ã‚‹ï¼‰
    top_k = st.slider("ä»¶æ•°ï¼ˆæœ€å¤§è¨ˆç®—ä»¶æ•°ï¼‰", 50, 500, 200, step=50)
    st.caption("â€»åˆå›ã¯èª­ã¿è¾¼ã¿ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¾Œã¯é€Ÿããªã‚Šã¾ã™ï¼‰")

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
with st.spinner("ã‚·ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™â€¦"):
    df = load_all_data_v2()

# --- ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¡¨ç¤ºï¼šãƒ­ã‚°ãŒç„¡ãã¦ã‚‚ df ã‹ã‚‰å¾©å…ƒã—ã¦å¿…ãšå‡ºã™ ---
sources = st.session_state.get("OPENED_LOG", [])
if (not sources) and (len(df) > 0):
    tmp = (
        df[["ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ", "ãƒ•ã‚¡ã‚¤ãƒ«ID"]]
        .dropna()
        .drop_duplicates()
        .values
        .tolist()
    )
    sources = [(title, sid) for title, sid in tmp]

with st.expander(f"ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼ˆ{len(sources)}ä»¶ï¼‰", expanded=False):
    st.caption(f"ğŸ“„ èª­ã¿è¾¼ã‚ãŸãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(df)}")
    for title, sid in sources:
        if isinstance(title, str) and title.startswith("âŒ"):
            st.caption(title)
        else:
            st.caption(f"âœ… {title} ({_short_id(sid)})")

# =============================================================================
# ğŸ“‘ ã‚·ãƒ¼ãƒˆåˆ¥ä¸€è¦§ï¼ˆä¸€è¦§ãƒ¢ãƒ¼ãƒ‰ï¼‰
# =============================================================================
if mode == "ğŸ“‘ ã‚·ãƒ¼ãƒˆåˆ¥ä¸€è¦§":
    st.subheader("ã‚·ãƒ¼ãƒˆåˆ¥ä¸€è¦§")

    # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®ãƒ—ãƒ«ãƒ€ã‚¦ãƒ³ï¼ˆã™ã¹ã¦ or å€‹åˆ¥ï¼‰
    ss_options = (
        df[["ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ", "ãƒ•ã‚¡ã‚¤ãƒ«ID"]]
        .dropna()
        .drop_duplicates()
        .sort_values("ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ")
        .values
        .tolist()
    )
    ss_names = ["ï¼ˆã™ã¹ã¦ï¼‰"] + [name for name, _id in ss_options]
    selected_ss = st.selectbox("ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’é¸æŠ", ss_names, index=0)

    if selected_ss == "ï¼ˆã™ã¹ã¦ï¼‰":
        df_view = df
    else:
        df_view = df[df["ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ"] == selected_ss]

    # ã‚·ãƒ¼ãƒˆå˜ä½ã«é›†è¨ˆï¼ˆä»¶æ•°ã¯è¨ˆç®—ã™ã‚‹ãŒè¡¨ç¤ºã«ã¯ä½¿ã‚ãªã„ï¼‰
    grp = (
        df_view.groupby(["ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ", "ãƒ•ã‚¡ã‚¤ãƒ«ID", "ã‚·ãƒ¼ãƒˆå", "ã‚·ãƒ¼ãƒˆGID"])
              .size().reset_index(name="ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°")
              .sort_values(["ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ", "ã‚·ãƒ¼ãƒˆå"])
    )

    if grp.empty:
        st.info("è©²å½“ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        st.stop()

    # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã”ã¨ã«ã¾ã¨ã‚ã¦è¡¨ç¤º
    for (ss_name, file_id), chunk in grp.groupby(["ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ", "ãƒ•ã‚¡ã‚¤ãƒ«ID"]):
        with st.expander(f"{ss_name}ï¼ˆ{len(chunk)}ã‚·ãƒ¼ãƒˆï¼‰", expanded=False):

            # å„ã‚·ãƒ¼ãƒˆã®è¡Œ
            for _, r in chunk.iterrows():
                gid = r["ã‚·ãƒ¼ãƒˆGID"]
                if pd.notna(gid):
                    url = f"https://docs.google.com/spreadsheets/d/{file_id}/edit#gid={int(gid)}"
                else:
                    url = f"https://docs.google.com/spreadsheets/d/{file_id}/edit"

                # â–¼ ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¯ã€Œãƒ†ãƒ¼ãƒã€ã‚’é‡è¤‡é™¤å»ã—ã¦ä¸Šä½è¡¨ç¤º
                themes_series = (
                    df[(df["ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ"] == ss_name) & (df["ã‚·ãƒ¼ãƒˆå"] == r["ã‚·ãƒ¼ãƒˆå"])]
                    ["ãƒ†ãƒ¼ãƒ"]
                    .fillna("")
                    .map(normalize)
                )
                # ç©ºã‚’é™¤å¤–ã—ã¦é‡è¤‡æ’é™¤
                themes_uniq = []
                seen = set()
                for t in themes_series:
                    if t and t not in seen:
                        seen.add(t)
                        themes_uniq.append(t)
                    if len(themes_uniq) >= 3:   # è¡¨ç¤ºæ•°ã¯å¿…è¦ã«å¿œã˜ã¦èª¿æ•´
                        break

                # è¡Œã®è¡¨ç¤ºï¼ˆãƒªãƒ³ã‚¯ï¼‹ãƒ†ãƒ¼ãƒã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰
                st.markdown(f"- [{r['ã‚·ãƒ¼ãƒˆå']}]({url})")
                if themes_uniq:
                    st.caption(" ï¼ ".join(themes_uniq))

    st.stop()  # ä¸€è¦§ãƒ¢ãƒ¼ãƒ‰ã§ã¯ã“ã“ã§çµ‚äº†

# =============================================================================
# ğŸ” æ¤œç´¢ï¼ˆæ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ï¼‰
# =============================================================================
st.caption("ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦æ¤œç´¢")
q = st.text_input(
    label="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›",
    value="",
    placeholder="ä¾‹: ç™ºè¡¨ç·´ç¿’, ã‚°ãƒ«ãƒ¼ãƒ—æ´»å‹•, æœ—èª­, å·¥ä½œ, è¡¨ç¾åŠ› ãªã©",
    label_visibility="collapsed",
)

# æ¤œç´¢èªãŒå¤‰ã‚ã£ãŸã‚‰è¡¨ç¤ºä»¶æ•°ã‚’ãƒªã‚»ãƒƒãƒˆ
if q != st.session_state.last_query:
    st.session_state.show_n = 15
    st.session_state.last_query = q

if q:
    # æ¤œç´¢æº–å‚™ï¼ˆåŸ‹ã‚è¾¼ã¿ï¼‹BM25ï¼‰â€»æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ã§ã®ã¿è¨ˆç®—
    def tokenize_ja(text: str) -> List[str]:
        text = normalize(text)
        toks = re.split(r"[ \u3000ã€ã€‚ãƒ»,./!?ï¼ï¼Ÿ\-\n\r\t]+", text)
        return [t for t in toks if t]

    @st.cache_data(show_spinner=False)
    def build_bm25(corpus_tokens: List[List[str]]):
        return BM25Okapi(corpus_tokens)

    @st.cache_resource(show_spinner=True)
    def load_embedder():
        return SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

    corpus_texts = (df["æ¤œç´¢ç”¨ãƒ†ã‚­ã‚¹ãƒˆ"].fillna("") + " " + df["ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å"].fillna("")).tolist()
    corpus_tokens = [tokenize_ja(t) for t in corpus_texts]
    bm25 = build_bm25(corpus_tokens)
    embedder = load_embedder()
    corpus_emb = embedder.encode(corpus_texts, normalize_embeddings=True, show_progress_bar=False)

    SYNONYMS = {
        "ç™ºè¡¨": ["ãƒ—ãƒ¬ã‚¼ãƒ³", "ã‚¹ãƒ”ãƒ¼ãƒ", "è¡¨ç¾", "ç™ºè¡¨ç·´ç¿’"],
        "è¡¨ç¾": ["ç™ºè¡¨", "ä¼ãˆã‚‹", "ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆ"],
        "å”åŠ›": ["å”åƒ", "ãƒãƒ¼ãƒ ", "ã‚°ãƒ«ãƒ¼ãƒ—", "å…±åŒ"],
        "å‰µä½œ": ["ã‚‚ã®ã¥ãã‚Š", "åˆ¶ä½œ", "å·¥ä½œ", "ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–"],
        "èª­è§£": ["èª­ã¿å–ã‚Š", "æ„Ÿæƒ³", "èª­æ›¸", "æœ—èª­"],
    }

    expanded = [q]
    for k, vs in SYNONYMS.items():
        if k in q:
            expanded += vs
    q_expanded = " ".join(expanded)

    q_toks = tokenize_ja(q_expanded)
    bm25_scores = bm25.get_scores(q_toks)

    q_emb = embedder.encode([q_expanded], normalize_embeddings=True, show_progress_bar=False)
    sem_scores = util.cos_sim(q_emb, corpus_emb).cpu().numpy()[0]

    def minmax(x):
        x = np.array(x, dtype=float)
        if x.max() - x.min() < 1e-9:
            return np.zeros_like(x)
        return (x - x.min()) / (x.max() - x.min())

    bm25_n = minmax(bm25_scores)
    sem_n  = minmax(sem_scores)
    final  = alpha * sem_n + (1 - alpha) * bm25_n

    # ä¸Šä½ top_k ã¾ã§å–å¾—ï¼ˆã“ã“ã¾ã§æ®µéšè¡¨ç¤ºã§å¢—ã‚„ã›ã‚‹ï¼‰
    idx_all = np.argsort(final)[::-1][:top_k]

    # ã„ã¾è¡¨ç¤ºã™ã‚‹ä»¶æ•°ï¼ˆ15ä»¶ãšã¤å¢—ãˆã‚‹ï¼‰
    show_n = min(st.session_state.show_n, len(idx_all))
    idx = idx_all[:show_n]

    st.subheader(f"æ¤œç´¢çµæœï¼ˆ{len(idx_all)}ä»¶ä¸­ {show_n}ä»¶ã‚’è¡¨ç¤ºï¼‰")

    for rank, i in enumerate(idx, start=1):
        row = df.iloc[i]

        # gid ãŒã‚ã‚Œã°ç‰¹å®šã‚¿ãƒ–ã¸ç›´ãƒªãƒ³ã‚¯
        gid = row.get("ã‚·ãƒ¼ãƒˆGID")
        fid = row["ãƒ•ã‚¡ã‚¤ãƒ«ID"]
        if pd.notna(gid):
            url = f"https://docs.google.com/spreadsheets/d/{fid}/edit#gid={int(gid)}"
        else:
            url = f"https://docs.google.com/spreadsheets/d/{fid}/edit"

        with st.container(border=True):
            st.markdown(
                f"**{rank}. {row.get('ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å','(åç§°æœªè¨­å®š)')}** ã€€"
                f"[{row['ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ']} / {row['ã‚·ãƒ¼ãƒˆå']}]({url})"
            )
            cols = st.columns(3)
            with cols[0]:
                st.write("**ãƒ†ãƒ¼ãƒ**:", row.get("ãƒ†ãƒ¼ãƒ",""))
                st.write("**å¯¾è±¡**:", row.get("å¯¾è±¡",""))
                st.write("**å‚åŠ äººæ•°**:", row.get("å‚åŠ äººæ•°",""))
            with cols[1]:
                st.write("**æº–å‚™ç‰©**:", row.get("æº–å‚™ç‰©",""))
                st.write("**å®Ÿæ–½æ–¹æ³•**:", row.get("å®Ÿæ–½æ–¹æ³•",""))
            with cols[2]:
                st.write("**å­ä¾›ãŸã¡ã®åå¿œ**:", row.get("å­ä¾›ãŸã¡ã®åå¿œ",""))
                st.write("**è‰¯ã‹ã£ãŸç‚¹**:", row.get("è‰¯ã‹ã£ãŸç‚¹",""))
                st.write("**æ”¹å–„ç‚¹**:", row.get("æ”¹å–„ç‚¹",""))
            st.caption(f"score={final[i]:.3f} / semantic={sem_n[i]:.3f} / bm25={bm25_n[i]:.3f}")

    # --- ã•ã‚‰ã«è¡¨ç¤ºãƒœã‚¿ãƒ³ ---
    if show_n < len(idx_all):
        c1, c2, _ = st.columns([1, 1, 6])
        if c1.button("ã•ã‚‰ã«è¡¨ç¤ºï¼ˆ+15ï¼‰"):
            st.session_state.show_n = min(show_n + 15, len(idx_all))
            st.rerun()
        if c2.button("å…¨ä»¶è¡¨ç¤º"):
            st.session_state.show_n = len(idx_all)
            st.rerun()



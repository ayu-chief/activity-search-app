import streamlit as st
import pandas as pd
import gspread
from gspread.exceptions import APIError
from google.oauth2.service_account import Credentials
from sentence_transformers import SentenceTransformer, util
from rank_bm25 import BM25Okapi
import re, unicodedata

st.set_page_config(page_title="ğŸ¯ æ´»å‹•ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¤œç´¢", layout="wide")

# =========================
#  Google Sheets æ¥ç¶šè¨­å®šï¼ˆçµ±ä¸€ï¼‰
# =========================
SERVICE_ACCOUNT_INFO = st.secrets["google_service_account"]
SPREADSHEET_IDS = [
    "1GCenO3IlDFrSITj1r90G_Vz_11D66POc8ny9HMtdCcM",
    "1Rjkgc6whTpg4FKUNLVSdzFSya-_tg42Wg4e10p-MmmI",
    "1PFDBuFuqxC4OWMCPjErP8uYYRovE55t-0oWsXNMCMqc",
    "1p4utUR9of_uSQNpzwJpSXgKiPrNur5nSTgHZvrbwmuc",
    "1HULvSdUAdSNdXXhPshu4mfwraf-bNq6zakFRhKF4Yfg",
]
SCOPES = [
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/drive",
]
creds = Credentials.from_service_account_info(SERVICE_ACCOUNT_INFO, scopes=SCOPES)
gc = gspread.authorize(creds)

# èªè¨¼ãƒ¡ãƒ¼ãƒ«ã‚’è¡¨ç¤ºï¼ˆç¢ºèªç”¨ï¼‰
st.info(f"Service Account: {creds.service_account_email}")

@st.cache_data(show_spinner=False)
def load_all_data():
    rows = []
    for sid in SPREADSHEET_IDS:
        try:
            st.write(f"ğŸ” Trying to open: {sid}")
            sh = gc.open_by_key(sid)
            st.success(f"âœ… Opened: {sh.title}")
        except APIError as e:
            # gspreadã®HTTPãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¡¨ç¤ºï¼ˆ403/404 ã®åˆ‡ã‚Šåˆ†ã‘ã«æœ‰ç”¨ï¼‰
            resp = getattr(e, "response", None)
            code = getattr(resp, "status_code", "?")
            text = getattr(resp, "text", str(e))
            st.error(f"âŒ Failed to open (status={code}): {sid}")
            st.code(text[:2000])
            continue
        except Exception as e:
            st.error(f"âŒ Failed to open (unexpected): {sid}")
            st.code(str(e))
            continue

        for ws in sh.worksheets():
            try:
                vals = ws.get_all_values()
            except Exception as e:
                st.error(f"âŒ Failed to read values: {sh.title} / {ws.title}")
                st.code(str(e))
                continue

            if not vals:
                continue
            rec = parse_sheet(vals)  # æ—¢å­˜ã®é–¢æ•°ã‚’åˆ©ç”¨
            if not any(rec.values()):
                continue

            rec["ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ"] = sh.title
            rec["ãƒ•ã‚¡ã‚¤ãƒ«ID"] = sid
            rec["ã‚·ãƒ¼ãƒˆå"] = ws.title
            rec["æ¤œç´¢ç”¨ãƒ†ã‚­ã‚¹ãƒˆ"] = " ".join([
                rec.get("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å",""), rec.get("ãƒ†ãƒ¼ãƒ",""), rec.get("å¯¾è±¡",""),
                rec.get("æº–å‚™ç‰©",""), rec.get("å®Ÿæ–½æ–¹æ³•",""),
                rec.get("å­ä¾›ãŸã¡ã®åå¿œ",""), rec.get("è‰¯ã‹ã£ãŸç‚¹",""), rec.get("æ”¹å–„ç‚¹","")
            ]).strip()
            rows.append(rec)

    return pd.DataFrame(rows)

# =========================
#  ãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ–/ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
# =========================
def normalize_text(s: str) -> str:
    if not isinstance(s, str):
        return ""
    s = unicodedata.normalize("NFKC", s).lower()
    s = re.sub(r"\s+", " ", s).strip()
    return s

def tokenize_ja_simple(s: str):
    s = normalize_text(s)
    s = re.sub(r"[ã€ã€‚ãƒ»,./!?:;()\[\]{}ã€Œã€ã€ã€ï¿¥$%^&*<>ï¼=ï¼‹+ï¼\-â€¦~â€•ãƒ¼]", " ", s)
    return [t for t in s.split(" ") if t]

# =========================
#  ã‚¯ã‚¨ãƒªæ‹¡å¼µï¼ˆåŒç¾©èªï¼‰
# =========================
SYNONYMS = {
    "æ¢ç©¶": ["ç·åˆå­¦ç¿’", "èª²é¡Œç ”ç©¶", "pbl", "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå­¦ç¿’"],
    "ç™ºè¡¨": ["ãƒ—ãƒ¬ã‚¼ãƒ³", "ã‚¹ãƒ”ãƒ¼ãƒ", "å£é ­ç™ºè¡¨", "ã‚·ãƒ§ãƒ¼ã‚±ãƒ¼ã‚¹"],
    "æŒ¯ã‚Šè¿”ã‚Š": ["ãƒªãƒ•ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³", "æ„Ÿæƒ³", "ãµã‚Šã‹ãˆã‚Š", "å†…çœ"],
    "è·æ¥­ä½“é¨“": ["ã‚­ãƒ£ãƒªã‚¢æ•™è‚²", "è·æ¥­è¬›è©±", "è·æ¥­æ¢ç©¶", "ã‚²ã‚¹ãƒˆè¬›æ¼”"],
    "ã‚°ãƒ«ãƒ¼ãƒ—æ´»å‹•": ["å”åƒå­¦ç¿’", "ãƒãƒ¼ãƒ ä½œæ¥­", "å…±åŒä½œæ¥­", "ç­æ´»å‹•"],
    "å‰µä½œ": ["åˆ¶ä½œ", "ã‚‚ã®ã¥ãã‚Š", "ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—", "ãƒãƒ³ãƒ‰ãƒ¡ã‚¤ãƒ‰"],
    "è¡¨ç¾": ["æ¼”åŠ‡", "æœ—èª­", "ã‚»ãƒªãƒ•", "èº«ä½“è¡¨ç¾", "ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³"],
}
def expand_query(q: str) -> list[str]:
    qn = normalize_text(q)
    qs = {qn}
    for k, alts in SYNONYMS.items():
        if k in qn or any(a in qn for a in alts):
            qs.update([normalize_text(x) for x in [k, *alts]])
    return list(qs)

# =========================
#  ã‚·ãƒ¼ãƒˆ â†’ 1ãƒ¬ã‚³ãƒ¼ãƒ‰æŠ½å‡º
#   ï¼ˆãƒ©ãƒ™ãƒ«è¡Œã‹ã‚‰å€¤ã‚’ã‹ãé›†ã‚ã‚‹ï¼‰
# =========================
LABELS = [
    "æ ¡èˆå","ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å","ãƒ†ãƒ¼ãƒ","å¯¾è±¡ç”Ÿå¾’","å‚åŠ äººæ•°",
    "æº–å‚™ç‰©","å®Ÿæ–½æ–¹æ³•","å­ä¾›ãŸã¡ã®åå¿œ","å­ã©ã‚‚ãŸã¡ã®åå¿œ","è‰¯ã‹ã£ãŸç‚¹","æ”¹å–„ç‚¹"
]

def extract_value(values, label):
    # ã‚·ãƒ¼ãƒˆå…¨ä½“ã‹ã‚‰ label ã‚’å«ã‚€ã‚»ãƒ«ã‚’æ¢ã—ã€ãã®è¡Œã®ã€Œå³å´ã‚»ãƒ«ç¾¤ã€ã‚’çµåˆã—ã¦è¿”ã™
    for row in values:
        for j, cell in enumerate(row):
            if label in str(cell):
                right = row[j+1:] if j+1 < len(row) else []
                toks = [t for t in right if t and str(t).strip()]
                if toks:
                    return " ".join(toks).strip()
    return ""

def parse_sheet(values):
    rec = {}
    for lab in LABELS:
        rec[lab] = extract_value(values, lab)
    # å­ã©ã‚‚/å­ä¾› ã©ã¡ã‚‰ã‹ã§è£œå®Œ
    if not rec.get("å­ä¾›ãŸã¡ã®åå¿œ"):
        rec["å­ä¾›ãŸã¡ã®åå¿œ"] = rec.get("å­ã©ã‚‚ãŸã¡ã®åå¿œ","")
    # è¡¨ç¤ºç”¨ãƒ»æ¤œç´¢ç”¨ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ•´å½¢
    rec_out = {
        "æ ¡èˆå": rec.get("æ ¡èˆå",""),
        "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å": rec.get("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å",""),
        "ãƒ†ãƒ¼ãƒ": rec.get("ãƒ†ãƒ¼ãƒ",""),
        "å¯¾è±¡": rec.get("å¯¾è±¡ç”Ÿå¾’",""),
        "å‚åŠ äººæ•°": rec.get("å‚åŠ äººæ•°",""),
        "æº–å‚™ç‰©": rec.get("æº–å‚™ç‰©",""),
        "å®Ÿæ–½æ–¹æ³•": rec.get("å®Ÿæ–½æ–¹æ³•",""),
        "å­ä¾›ãŸã¡ã®åå¿œ": rec.get("å­ä¾›ãŸã¡ã®åå¿œ",""),
        "è‰¯ã‹ã£ãŸç‚¹": rec.get("è‰¯ã‹ã£ãŸç‚¹",""),
        "æ”¹å–„ç‚¹": rec.get("æ”¹å–„ç‚¹",""),
    }
    return rec_out

@st.cache_data(show_spinner=False)
def load_all_data():
    rows = []
    for sid in SPREADSHEET_IDS:
        sh = gc.open_by_key(sid)
        file_title = sh.title
        for ws in sh.worksheets():
            vals = ws.get_all_values()
            if not vals:
                continue
            rec = parse_sheet(vals)
            # ã»ã¼ç©ºã®ã‚·ãƒ¼ãƒˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if not any(rec.values()):
                continue
            rec["ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ"] = file_title
            rec["ãƒ•ã‚¡ã‚¤ãƒ«ID"] = sid
            rec["ã‚·ãƒ¼ãƒˆå"] = ws.title
            # æ¤œç´¢ç”¨ãƒ†ã‚­ã‚¹ãƒˆ
            rec["æ¤œç´¢ç”¨ãƒ†ã‚­ã‚¹ãƒˆ"] = " ".join([
                rec.get("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å",""), rec.get("ãƒ†ãƒ¼ãƒ",""), rec.get("å¯¾è±¡",""),
                rec.get("æº–å‚™ç‰©",""), rec.get("å®Ÿæ–½æ–¹æ³•",""),
                rec.get("å­ä¾›ãŸã¡ã®åå¿œ",""), rec.get("è‰¯ã‹ã£ãŸç‚¹",""), rec.get("æ”¹å–„ç‚¹","")
            ]).strip()
            rows.append(rec)
    return pd.DataFrame(rows)

# =========================
#  ãƒ¢ãƒ‡ãƒ«&ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æº–å‚™
# =========================
@st.cache_resource(show_spinner=True)
def load_model():
    return SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
model = load_model()

df = load_all_data()
if df.empty:
    st.error("ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®å…±æœ‰æ¨©é™ã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
    st.stop()

# æ­£è¦åŒ–ã‚³ãƒ¼ãƒ‘ã‚¹
df["æ¤œç´¢ç”¨ãƒ†ã‚­ã‚¹ãƒˆ_norm"] = df["æ¤œç´¢ç”¨ãƒ†ã‚­ã‚¹ãƒˆ"].map(normalize_text)

# 1) ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯åŸ‹ã‚è¾¼ã¿
@st.cache_resource(show_spinner=True)
def build_embeddings(texts: list[str]):
    return model.encode(texts, convert_to_tensor=True)
corpus_embeddings = build_embeddings(df["æ¤œç´¢ç”¨ãƒ†ã‚­ã‚¹ãƒˆ_norm"].tolist())

# 2) BM25
corpus_tokens = [tokenize_ja_simple(t) for t in df["æ¤œç´¢ç”¨ãƒ†ã‚­ã‚¹ãƒˆ_norm"].tolist()]
bm25 = BM25Okapi(corpus_tokens)

# =========================
#  ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ï¼ˆå¿…ãšè¿”ã™ï¼‰
# =========================
def hybrid_search(query: str, top_k: int = 8, alpha: float = 0.7):
    # A) ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯
    q_emb = model.encode(normalize_text(query), convert_to_tensor=True)
    sem_hits = util.semantic_search(q_emb, corpus_embeddings, top_k=max(50, top_k*5))[0]
    sem_scores = {h["corpus_id"]: float(h["score"]) for h in sem_hits}

    # B) ã‚¯ã‚¨ãƒªæ‹¡å¼µ + BM25ï¼ˆæœ€å¤§å€¤ã§çµ±åˆï¼‰
    bm25_scores = {}
    for qx in (expand_query(query) or [query]):
        toks = tokenize_ja_simple(qx)
        scores = bm25.get_scores(toks)
        for i, s in enumerate(scores):
            bm25_scores[i] = max(bm25_scores.get(i, 0.0), float(s))

    # C) BM25ã‚¹ã‚³ã‚¢æ­£è¦åŒ–
    max_bm = max(bm25_scores.values(), default=1.0) or 1.0
    bm25_scores = {i: s / max_bm for i, s in bm25_scores.items()}

    # D) çµåˆï¼ˆã—ãã„å€¤ã§å¼¾ã‹ãªã„ï¼‰
    all_ids = set(sem_scores) | set(bm25_scores)
    merged = []
    for i in all_ids:
        s_sem = sem_scores.get(i, 0.0)
        s_bm  = bm25_scores.get(i, 0.0)
        score = alpha * s_sem + (1 - alpha) * s_bm
        merged.append((i, score, s_sem, s_bm))
    merged.sort(key=lambda x: x[1], reverse=True)
    return merged[:top_k]

# =========================
#  UI
# =========================
st.markdown("""
<div style='background:#e3f3ec;padding:16px;border-radius:8px'>
  <h2 style='margin:0'>ğŸ¯ æ´»å‹•ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¤œç´¢</h2>
  <p style='margin:4px 0 0'>èªãŒå«ã¾ã‚Œã¦ã„ãªãã¦ã‚‚ã€æ„å‘³ãŒè¿‘ã„æ´»å‹•ã‚’å¿…ãšæç¤ºã—ã¾ã™ã€‚</p>
</div>
""", unsafe_allow_html=True)
st.write("")

col1, col2, col3 = st.columns([3,1,1])
with col1:
    query = st.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚„ç›®çš„ï¼ˆä¾‹ï¼šç™ºè¡¨ç·´ç¿’ / ç‰©èªä½“é¨“ / ã‚°ãƒ«ãƒ¼ãƒ—æ´»å‹• ãªã©ï¼‰", "")
with col2:
    top_k = st.number_input("ä»¶æ•°", 1, 20, 6)
with col3:
    alpha = st.slider("æ„å‘³é‡è¦–â†”èªä¸€è‡´", 0.0, 1.0, 0.7, 0.05)

# å¯¾è±¡ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆä»»æ„ï¼‰
targets = st.multiselect("å¯¾è±¡ã§çµã‚Šè¾¼ã¿ï¼ˆä»»æ„ï¼‰", ["å°", "ä¸­", "é«˜"])

def pass_target_filter(text):
    if not targets:
        return True
    t = text or ""
    return all(any(tag in t for tag in targets) for tag in targets)

if query:
    results = hybrid_search(query, top_k=top_k, alpha=alpha)
    st.caption("æ¤œç´¢æ–¹å¼: Semantic + BM25 + ã‚¯ã‚¨ãƒªæ‹¡å¼µï¼ˆã—ãã„å€¤ãªã—ãƒ»å¿…ãšææ¡ˆï¼‰")
else:
    # æœªå…¥åŠ›æ™‚ã¯ãŠã™ã™ã‚è¡¨ç¤º
    results = [(i, 0.0, 0.0, 0.0) for i in df.sample(min(top_k, len(df)), random_state=0).index]
    st.caption("æœªå…¥åŠ›ã®ãŸã‚ã€ãƒ©ãƒ³ãƒ€ãƒ ã«ãŠã™ã™ã‚ã‚’è¡¨ç¤ºä¸­")

shown = 0
for i, total, s_sem, s_bm in results:
    row = df.iloc[i]
    if not pass_target_filter(row.get("å¯¾è±¡","")):
        continue
    st.markdown(f"### ğŸ“Œ {row.get('ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å','(åç§°æœªè¨­å®š)')}ã€€â€”ã€€{row.get('ã‚·ãƒ¼ãƒˆå','')}")
    if query:
        st.progress(min(1.0, total))
        st.caption(f"(semantic: {s_sem:.2f} / bm25: {s_bm:.2f} / total: {total:.2f})")
    cols = st.columns(3)
    with cols[0]:
        st.write(f"**ãƒ†ãƒ¼ãƒ:** {row.get('ãƒ†ãƒ¼ãƒ','')}")
        st.write(f"**å¯¾è±¡:** {row.get('å¯¾è±¡','')}")
        st.write(f"**å‚åŠ äººæ•°:** {row.get('å‚åŠ äººæ•°','')}")
    with cols[1]:
        st.write(f"**æº–å‚™ç‰©:** {row.get('æº–å‚™ç‰©','')}")
        st.write(f"**è‰¯ã‹ã£ãŸç‚¹:** {row.get('è‰¯ã‹ã£ãŸç‚¹','')}")
    with cols[2]:
        st.write(f"**å®Ÿæ–½æ–¹æ³•:** {row.get('å®Ÿæ–½æ–¹æ³•','')}")
        st.write(f"**å­ä¾›ãŸã¡ã®åå¿œ:** {row.get('å­ä¾›ãŸã¡ã®åå¿œ','')}")
        st.write(f"**æ”¹å–„ç‚¹:** {row.get('æ”¹å–„ç‚¹','')}")
    st.caption(f"ğŸ“„ {row.get('ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ','')} / ã‚·ãƒ¼ãƒˆ: {row.get('ã‚·ãƒ¼ãƒˆå','')}")
    st.divider()
    shown += 1

if shown == 0:
    st.info("è©²å½“ãŒãƒ•ã‚£ãƒ«ã‚¿ã§é™¤å¤–ã•ã‚Œã¾ã—ãŸã€‚ãƒ•ã‚£ãƒ«ã‚¿ã‚„ä»¶æ•°ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")




# app.py
import time
import re
import unicodedata
from typing import List, Dict

import streamlit as st
import pandas as pd
import gspread
from gspread.exceptions import APIError
from google.oauth2.service_account import Credentials

from sentence_transformers import SentenceTransformer, util
from rank_bm25 import BM25Okapi
import numpy as np

# -----------------------------------------------------------------------------
# åŸºæœ¬è¨­å®š
# -----------------------------------------------------------------------------
st.set_page_config(page_title="æ´»å‹•ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¤œç´¢", layout="wide")

# èª­ã¿è¾¼ã¿çµæœã®æ§ãˆã‚è¡¨ç¤ºç”¨ãƒ­ã‚°ï¼ˆExpanderã«ã¾ã¨ã‚ã‚‹ï¼‰
if "OPENED_LOG" not in st.session_state:
    st.session_state.OPENED_LOG = []  # [(title, sid), ...]

# -----------------------------------------------------------------------------
# Google Sheets æ¥ç¶š
# -----------------------------------------------------------------------------
SERVICE_ACCOUNT_INFO = st.secrets["google_service_account"]  # Secretsã«ä¿å­˜ã—ãŸJSON
SPREADSHEET_IDS = [
    "1GCenO3IlDFrSITj1r90G_Vz_11D66POc8ny9HMtdCcM",
    "1Rjkgc6whTpg4FKUNLVSdzFSya-_tg42Wg4e10p-MmmI",
    "1PFDBuFuqxC4OWMCPjErP8uYYRovE55t-0oWsXNMCMqc",
    "1p4utUR9of_uSQNpzwJpSXgKiPrNur5nSTgHZvrbwmuc",
    "1HULvSdUAdSNdXXhPshu4mfwraf-bNq6zakFRhKF4Yfg",
]
SCOPES = [
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/drive",
]
creds = Credentials.from_service_account_info(SERVICE_ACCOUNT_INFO, scopes=SCOPES)
gc = gspread.authorize(creds)

# ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼ˆåºƒã‚æ¨å¥¨ï¼‰
BASE_WAIT = 8.0   # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆé–“ã®å¾…æ©Ÿï¼ˆç§’ï¼‰
MAX_RETRY = 6     # 429æ™‚ã®æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°

# -----------------------------------------------------------------------------
# æ­£è¦åŒ–ãƒ˜ãƒ«ãƒ‘
# -----------------------------------------------------------------------------
def normalize(s: str) -> str:
    if s is None:
        return ""
    s = unicodedata.normalize("NFKC", str(s))
    s = re.sub(r"\s+", " ", s).strip()
    return s

# -----------------------------------------------------------------------------
# ã‚·ãƒ¼ãƒˆ1æš â†’ ãƒ¬ã‚³ãƒ¼ãƒ‰åŒ–ï¼ˆãƒ©ãƒ™ãƒ«å–ã‚Šå‡ºã—ï¼‰
# -----------------------------------------------------------------------------
LABELS = [
    "æ ¡èˆå", "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å", "ãƒ†ãƒ¼ãƒ", "å¯¾è±¡ç”Ÿå¾’", "å¯¾è±¡", "å‚åŠ äººæ•°",
    "æº–å‚™ç‰©", "å®Ÿæ–½æ–¹æ³•", "å­ä¾›ãŸã¡ã®åå¿œ", "å­ã©ã‚‚ãŸã¡ã®åå¿œ", "è‰¯ã‹ã£ãŸç‚¹", "æ”¹å–„ç‚¹",
]

def extract_value(values: List[List[str]], label: str) -> str:
    lab = normalize(label)
    for row in values:
        for j, cell in enumerate(row):
            if lab and lab in normalize(cell):
                right = row[j+1:] if j+1 < len(row) else []
                toks = [c for c in right if str(c).strip()]
                if toks:
                    return " ".join(normalize(c) for c in toks).strip()
    return ""

def parse_sheet(values: List[List[str]]) -> Dict[str, str]:
    rec = {}
    for lab in LABELS:
        rec[lab] = extract_value(values, lab)

    # å­ã©ã‚‚/å­ä¾› ã‚’çµ±ä¸€
    if not rec.get("å­ä¾›ãŸã¡ã®åå¿œ"):
        rec["å­ä¾›ãŸã¡ã®åå¿œ"] = rec.get("å­ã©ã‚‚ãŸã¡ã®åå¿œ", "")

    out = {
        "æ ¡èˆå": rec.get("æ ¡èˆå", ""),
        "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å": rec.get("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å", ""),
        "ãƒ†ãƒ¼ãƒ": rec.get("ãƒ†ãƒ¼ãƒ", ""),
        "å¯¾è±¡": rec.get("å¯¾è±¡ç”Ÿå¾’", "") or rec.get("å¯¾è±¡", ""),
        "å‚åŠ äººæ•°": rec.get("å‚åŠ äººæ•°", ""),
        "æº–å‚™ç‰©": rec.get("æº–å‚™ç‰©", ""),
        "å®Ÿæ–½æ–¹æ³•": rec.get("å®Ÿæ–½æ–¹æ³•", ""),
        "å­ä¾›ãŸã¡ã®åå¿œ": rec.get("å­ä¾›ãŸã¡ã®åå¿œ", ""),
        "è‰¯ã‹ã£ãŸç‚¹": rec.get("è‰¯ã‹ã£ãŸç‚¹", ""),
        "æ”¹å–„ç‚¹": rec.get("æ”¹å–„ç‚¹", ""),
    }

    # ä¸»è¦ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒç©ºãªã‚‰ä¿é™ºã¨ã—ã¦å…¨ã‚»ãƒ«é€£çµ
    if not any(out.values()):
        flat = " ".join([" ".join([str(c) for c in r if str(c).strip()]) for r in values])
        out["ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å"] = out["ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å"] or "(åç§°æœªè¨­å®š)"
        out["ãƒ†ãƒ¼ãƒ"] = flat[:200]

    return out

# -----------------------------------------------------------------------------
# 429å›é¿ï¼šworksheets() ã‚’ä½¿ã‚ãšã€metadata(title)â†’values.batchGet
# -----------------------------------------------------------------------------
def _short_id(sid: str) -> str:
    return f"{sid[:6]}â€¦{sid[-4:]}" if len(sid) > 12 else sid

def open_sheet_by_id(sid: str):
    """é–‹ã‘ãŸã‚‰ãƒ­ã‚°ã«è¿½åŠ ï¼ˆç”»é¢ã«ã¯ãã®å ´ã§å‡ºã•ãªã„ï¼‰"""
    for attempt in range(MAX_RETRY):
        try:
            sh = gc.open_by_key(sid)
            st.session_state.OPENED_LOG.append((sh.title, sid))
            return sh
        except APIError as e:
            code = getattr(getattr(e, "response", None), "status_code", None)
            if code == 429 and attempt < MAX_RETRY - 1:
                wait = BASE_WAIT * (2 ** attempt)
                st.warning(f"â³ Rate limit: open_by_key (retry {attempt+1}/{MAX_RETRY}) in {wait:.1f}s")
                time.sleep(wait)
                continue
            st.session_state.OPENED_LOG.append((f"âŒ FAILED: {sid}", sid))
            return None
        except Exception:
            st.session_state.OPENED_LOG.append((f"âŒ FAILED: {sid}", sid))
            return None

@st.cache_data(show_spinner=True, ttl=6*60*60)
def load_all_data_v2() -> pd.DataFrame:
    """worksheets()ã¯ä½¿ã‚ãšã€ã‚¿ã‚¤ãƒˆãƒ«ã ã‘å–å¾—â†’values.batchGetã§ä¸€æ‹¬å–å¾—"""
    rows = []
    for sid in SPREADSHEET_IDS:
        sh = open_sheet_by_id(sid)
        if not sh:
            continue

        # 1) ã‚¿ã‚¤ãƒˆãƒ«ã®ã¿è»½é‡å–å¾—
        meta = None
        for attempt in range(MAX_RETRY):
            try:
                meta = sh.fetch_sheet_metadata(params={"fields": "sheets(properties(title))"})
                break
            except APIError as e:
                code = getattr(getattr(e, "response", None), "status_code", None)
                if code == 429 and attempt < MAX_RETRY - 1:
                    wait = BASE_WAIT * (2 ** attempt)
                    st.warning(f"â³ Rate limit: fetch_sheet_metadata (retry {attempt+1}/{MAX_RETRY}) in {wait:.1f}s")
                    time.sleep(wait)
                    continue
                meta = None
                break
            except Exception:
                meta = None
                break
        if not meta:
            continue

        titles = [s["properties"]["title"] for s in meta.get("sheets", []) if "properties" in s]
        # åˆ—å¹…ã¯å¿…è¦ã«å¿œã˜ã¦ç‹­ã‚ã‚‹ï¼ˆA:N ãªã©ï¼‰ã€‚ç‹­ã„ã»ã©é€Ÿã„
        ranges = [f"'{t}'!A:Q" for t in titles]

        # 2) ä¸€æ‹¬å–å¾—ï¼ˆvalues.batchGetï¼‰
        time.sleep(BASE_WAIT)
        vals_resp = None
        for attempt in range(MAX_RETRY):
            try:
                vals_resp = sh.values_batch_get(ranges=ranges, params={"majorDimension": "ROWS"})
                break
            except APIError as e:
                code = getattr(getattr(e, "response", None), "status_code", None)
                if code == 429 and attempt < MAX_RETRY - 1:
                    wait = BASE_WAIT * (2 ** attempt)
                    st.warning(f"â³ Rate limit: values.batchGet (retry {attempt+1}/{MAX_RETRY}) in {wait:.1f}s")
                    time.sleep(wait)
                    continue
                vals_resp = None
                break
            except Exception:
                vals_resp = None
                break
        if not vals_resp:
            continue

        for vr in vals_resp.get("valueRanges", []):
            rng = vr.get("range", "")
            ws_title = rng.split("!")[0].strip("'")
            vals = vr.get("values", [])
            if not vals:
                continue

            rec = parse_sheet(vals)
            if not any(rec.values()):
                continue

            rec["ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ"] = sh.title
            rec["ãƒ•ã‚¡ã‚¤ãƒ«ID"] = sid
            rec["ã‚·ãƒ¼ãƒˆå"] = ws_title
            rec["æ¤œç´¢ç”¨ãƒ†ã‚­ã‚¹ãƒˆ"] = " ".join([
                rec.get("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å",""), rec.get("ãƒ†ãƒ¼ãƒ",""), rec.get("å¯¾è±¡",""),
                rec.get("æº–å‚™ç‰©",""), rec.get("å®Ÿæ–½æ–¹æ³•",""),
                rec.get("å­ä¾›ãŸã¡ã®åå¿œ",""), rec.get("è‰¯ã‹ã£ãŸç‚¹",""), rec.get("æ”¹å–„ç‚¹","")
            ]).strip()
            rows.append(rec)

    return pd.DataFrame(rows)

# -----------------------------------------------------------------------------
# æ¤œç´¢æº–å‚™ï¼ˆåŸ‹ã‚è¾¼ã¿ï¼‹BM25ï¼‰
# -----------------------------------------------------------------------------
@st.cache_resource(show_spinner=True)
def load_embedder():
    return SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

@st.cache_data(show_spinner=False)
def build_bm25(corpus_tokens: List[List[str]]):
    return BM25Okapi(corpus_tokens)

def tokenize_ja(text: str) -> List[str]:
    text = normalize(text)
    toks = re.split(r"[ \u3000ã€ã€‚ãƒ»,./!?ï¼ï¼Ÿ\-\n\r\t]+", text)
    return [t for t in toks if t]

SYNONYMS = {
    "ç™ºè¡¨": ["ãƒ—ãƒ¬ã‚¼ãƒ³", "ã‚¹ãƒ”ãƒ¼ãƒ", "è¡¨ç¾", "ç™ºè¡¨ç·´ç¿’"],
    "è¡¨ç¾": ["ç™ºè¡¨", "ä¼ãˆã‚‹", "ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆ"],
    "å”åŠ›": ["å”åƒ", "ãƒãƒ¼ãƒ ", "ã‚°ãƒ«ãƒ¼ãƒ—", "å…±åŒ"],
    "å‰µä½œ": ["ã‚‚ã®ã¥ãã‚Š", "åˆ¶ä½œ", "å·¥ä½œ", "ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–"],
    "èª­è§£": ["èª­ã¿å–ã‚Š", "æ„Ÿæƒ³", "èª­æ›¸", "æœ—èª­"],
}

# -----------------------------------------------------------------------------
# UI
# -----------------------------------------------------------------------------
st.title("æ´»å‹•ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¤œç´¢")

with st.sidebar:
    st.header("æ¤œç´¢è¨­å®š")
    alpha = st.slider("æ„å‘³é‡è¦–ï¼ˆ1.0ï¼‰ â†â†’ èªä¸€è‡´é‡è¦–ï¼ˆ0.0ï¼‰", 0.0, 1.0, 0.7, 0.05)
    top_k = st.slider("ä»¶æ•°", 5, 50, 15)
    st.caption("â€»åˆå›ã¯èª­ã¿è¾¼ã¿ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¾Œã¯é€Ÿããªã‚Šã¾ã™ï¼‰")

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
with st.spinner("ã‚·ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™â€¦"):
    df = load_all_data_v2()

# --- ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¡¨ç¤ºï¼šãƒ­ã‚°ãŒç„¡ãã¦ã‚‚ df ã‹ã‚‰å¾©å…ƒã—ã¦å¿…ãšå‡ºã™ ---
sources = st.session_state.get("OPENED_LOG", [])
if (not sources) and (len(df) > 0):
    tmp = (
        df[["ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ", "ãƒ•ã‚¡ã‚¤ãƒ«ID"]]
        .dropna()
        .drop_duplicates()
        .values
        .tolist()
    )
    sources = [(title, sid) for title, sid in tmp]

with st.expander(f"ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼ˆ{len(sources)}ä»¶ï¼‰", expanded=False):
    st.caption(f"ğŸ“„ èª­ã¿è¾¼ã‚ãŸãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(df)}")
    for title, sid in sources:
        if isinstance(title, str) and title.startswith("âŒ"):
            st.caption(title)
        else:
            st.caption(f"âœ… {title} ({_short_id(sid)})")

# æ¤œç´¢ã‚³ãƒ¼ãƒ‘ã‚¹
corpus_texts = (df["æ¤œç´¢ç”¨ãƒ†ã‚­ã‚¹ãƒˆ"].fillna("") + " " + df["ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å"].fillna("")).tolist()
corpus_tokens = [tokenize_ja(t) for t in corpus_texts]
bm25 = build_bm25(corpus_tokens)
embedder = load_embedder()
corpus_emb = embedder.encode(corpus_texts, normalize_embeddings=True, show_progress_bar=False)

# æ¤œç´¢ãƒ•ã‚©ãƒ¼ãƒ 
st.divider()
st.caption("ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦æ¤œç´¢")
q = st.text_input(
    label="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›",
    value="",
    placeholder="ä¾‹: ç™ºè¡¨ç·´ç¿’, ã‚°ãƒ«ãƒ¼ãƒ—æ´»å‹•, æœ—èª­, å·¥ä½œ, è¡¨ç¾åŠ› ãªã©",
    label_visibility="collapsed",
)

if q:
    expanded = [q]
    for k, vs in SYNONYMS.items():
        if k in q:
            expanded += vs
    q_expanded = " ".join(expanded)

    q_toks = tokenize_ja(q_expanded)
    bm25_scores = bm25.get_scores(q_toks)

    q_emb = embedder.encode([q_expanded], normalize_embeddings=True, show_progress_bar=False)
    sem_scores = util.cos_sim(q_emb, corpus_emb).cpu().numpy()[0]

    def minmax(x):
        x = np.array(x, dtype=float)
        if x.max() - x.min() < 1e-9:
            return np.zeros_like(x)
        return (x - x.min()) / (x.max() - x.min())

    bm25_n = minmax(bm25_scores)
    sem_n  = minmax(sem_scores)
    final  = alpha * sem_n + (1 - alpha) * bm25_n
    idx = np.argsort(final)[::-1][:top_k]

    st.subheader("æ¤œç´¢çµæœ")
    for rank, i in enumerate(idx, start=1):
        row = df.iloc[i]
        url = f"https://docs.google.com/spreadsheets/d/{row['ãƒ•ã‚¡ã‚¤ãƒ«ID']}/edit"
        with st.container(border=True):
            st.markdown(
                f"**{rank}. {row.get('ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å','(åç§°æœªè¨­å®š)')}** ã€€"
                f"[{row['ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ']} / {row['ã‚·ãƒ¼ãƒˆå']}]({url})"
            )
            cols = st.columns(3)
            with cols[0]:
                st.write("**ãƒ†ãƒ¼ãƒ**:", row.get("ãƒ†ãƒ¼ãƒ",""))
                st.write("**å¯¾è±¡**:", row.get("å¯¾è±¡",""))
                st.write("**å‚åŠ äººæ•°**:", row.get("å‚åŠ äººæ•°",""))
            with cols[1]:
                st.write("**æº–å‚™ç‰©**:", row.get("æº–å‚™ç‰©",""))
                st.write("**å®Ÿæ–½æ–¹æ³•**:", row.get("å®Ÿæ–½æ–¹æ³•",""))
            with cols[2]:
                st.write("**å­ä¾›ãŸã¡ã®åå¿œ**:", row.get("å­ä¾›ãŸã¡ã®åå¿œ",""))
                st.write("**è‰¯ã‹ã£ãŸç‚¹**:", row.get("è‰¯ã‹ã£ãŸç‚¹",""))
                st.write("**æ”¹å–„ç‚¹**:", row.get("æ”¹å–„ç‚¹",""))
            st.caption(f"score={final[i]:.3f} / semantic={sem_n[i]:.3f} / bm25={bm25_n[i]:.3f}")

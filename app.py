# app.py
# -*- coding: utf-8 -*-
import time
import re
import unicodedata
from typing import List, Dict

import streamlit as st
import pandas as pd
import numpy as np

import gspread
from gspread.exceptions import APIError
from google.oauth2.service_account import Credentials

from sentence_transformers import SentenceTransformer, util
from rank_bm25 import BM25Okapi

# =============================================================================
# åŸºæœ¬è¨­å®š
# =============================================================================
st.set_page_config(page_title="æ´»å‹•ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¤œç´¢", layout="wide")

# åˆæœŸåŒ–ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆï¼‰
if "OPENED_LOG" not in st.session_state:
    st.session_state.OPENED_LOG = []  # [(title, sid), ...]
if "show_n" not in st.session_state:
    st.session_state.show_n = 15      # æ¤œç´¢çµæœã®æ®µéšè¡¨ç¤º
if "last_query" not in st.session_state:
    st.session_state.last_query = ""

# =============================================================================
# Google Sheets æ¥ç¶šè¨­å®š
# =============================================================================
SERVICE_ACCOUNT_INFO = st.secrets["google_service_account"]
SPREADSHEET_IDS = [
    "1GCenO3IlDFrSITj1r90G_Vz_11D66POc8ny9HMtdCcM",
    "1Rjkgc6whTpg4FKUNLVSdzFSya-_tg42Wg4e10p-MmmI",
    "1PFDBuFuqxC4OWMCPjErP8uYYRovE55t-0oWsXNMCMqc",
    "1p4utUR9of_uSQNpzwJpSXgKiPrNur5nSTgHZvrbwmuc",
    "1HULvSdUAdSNdXXhPshu4mfwraf-bNq6zakFRhKF4Yfg",
]
SCOPES = [
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/drive",
]
creds = Credentials.from_service_account_info(SERVICE_ACCOUNT_INFO, scopes=SCOPES)
gc = gspread.authorize(creds)

# ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼ˆå¿…è¦ã«å¿œã˜ã¦èª¿æ•´ï¼‰
BASE_WAIT = 5.0   # 8.0 â†’ 5.0 ã«çŸ­ç¸®ï¼ˆ429 ãŒå‡ºã‚‹ã‚ˆã†ãªã‚‰æˆ»ã—ã¦ãã ã•ã„ï¼‰
MAX_RETRY = 6

# =============================================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =============================================================================
def normalize(s: str) -> str:
    if s is None:
        return ""
    s = unicodedata.normalize("NFKC", str(s))
    s = re.sub(r"\s+", " ", s).strip()
    return s

def _short_id(sid: str) -> str:
    return f"{sid[:6]}â€¦{sid[-4:]}" if len(sid) > 12 else sid

# -----------------------------------------------------------------------------
# ã‚·ãƒ¼ãƒˆ â†’ ãƒ¬ã‚³ãƒ¼ãƒ‰åŒ–
# -----------------------------------------------------------------------------
LABELS = [
    "æ ¡èˆå", "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å", "ãƒ†ãƒ¼ãƒ", "å¯¾è±¡ç”Ÿå¾’", "å¯¾è±¡", "å‚åŠ äººæ•°",
    "æº–å‚™ç‰©", "å®Ÿæ–½æ–¹æ³•", "å­ä¾›ãŸã¡ã®åå¿œ", "å­ã©ã‚‚ãŸã¡ã®åå¿œ", "è‰¯ã‹ã£ãŸç‚¹", "æ”¹å–„ç‚¹",
]

def extract_value(values: List[List[str]], label: str) -> str:
    lab = normalize(label)
    for row in values:
        for j, cell in enumerate(row):
            if lab and lab in normalize(cell):
                right = row[j + 1:] if j + 1 < len(row) else []
                toks = [c for c in right if str(c).strip()]
                if toks:
                    return " ".join(normalize(c) for c in toks).strip()
    return ""

def parse_sheet(values: List[List[str]]) -> Dict[str, str]:
    rec = {}
    for lab in LABELS:
        rec[lab] = extract_value(values, lab)

    # å­ã©ã‚‚/å­ä¾› ã‚’çµ±ä¸€
    if not rec.get("å­ä¾›ãŸã¡ã®åå¿œ"):
        rec["å­ä¾›ãŸã¡ã®åå¿œ"] = rec.get("å­ã©ã‚‚ãŸã¡ã®åå¿œ", "")

    out = {
        "æ ¡èˆå": rec.get("æ ¡èˆå", ""),
        "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å": rec.get("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å", ""),
        "ãƒ†ãƒ¼ãƒ": rec.get("ãƒ†ãƒ¼ãƒ", ""),
        "å¯¾è±¡": rec.get("å¯¾è±¡ç”Ÿå¾’", "") or rec.get("å¯¾è±¡", ""),
        "å‚åŠ äººæ•°": rec.get("å‚åŠ äººæ•°", ""),
        "æº–å‚™ç‰©": rec.get("æº–å‚™ç‰©", ""),
        "å®Ÿæ–½æ–¹æ³•": rec.get("å®Ÿæ–½æ–¹æ³•", ""),
        "å­ä¾›ãŸã¡ã®åå¿œ": rec.get("å­ä¾›ãŸã¡ã®åå¿œ", ""),
        "è‰¯ã‹ã£ãŸç‚¹": rec.get("è‰¯ã‹ã£ãŸç‚¹", ""),
        "æ”¹å–„ç‚¹": rec.get("æ”¹å–„ç‚¹", ""),
    }

    # ä¸»è¦ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒç©ºãªã‚‰ä¿é™ºã¨ã—ã¦å…¨ã‚»ãƒ«é€£çµ
    if not any(out.values()):
        flat = " ".join([" ".join([str(c) for c in r if str(c).strip()]) for r in values])
        out["ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å"] = out["ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å"] or "(åç§°æœªè¨­å®š)"
        out["ãƒ†ãƒ¼ãƒ"] = flat[:200]

    return out

# -----------------------------------------------------------------------------
# Google Sheets èª­ã¿è¾¼ã¿ï¼ˆ429 å›é¿ï¼šmetadataâ†’values.batchGetï¼‰
# -----------------------------------------------------------------------------
def open_sheet_by_id(sid: str):
    """é–‹ã‘ãŸã‚‰ãƒ­ã‚°ã«è¿½åŠ ï¼ˆç”»é¢ã«ã¯ãã®å ´ã§å‡ºã•ãªã„ï¼‰"""
    for attempt in range(MAX_RETRY):
        try:
            sh = gc.open_by_key(sid)
            st.session_state.OPENED_LOG.append((sh.title, sid))
            return sh
        except APIError as e:
            code = getattr(getattr(e, "response", None), "status_code", None)
            if code == 429 and attempt < MAX_RETRY - 1:
                wait = BASE_WAIT * (2 ** attempt)
                st.warning(f"â³ Rate limit: open_by_key (retry {attempt+1}/{MAX_RETRY}) in {wait:.1f}s")
                time.sleep(wait)
                continue
            st.session_state.OPENED_LOG.append((f"âŒ FAILED: {sid}", sid))
            return None
        except Exception:
            st.session_state.OPENED_LOG.append((f"âŒ FAILED: {sid}", sid))
            return None

@st.cache_data(show_spinner=True, ttl=24*3600)  # 24æ™‚é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def load_all_data_v2() -> pd.DataFrame:
    """worksheets()ã¯ä½¿ã‚ãšã€ã‚¿ã‚¤ãƒˆãƒ«ï¼†sheetIdã‚’å–å¾—â†’values.batchGetã§ä¸€æ‹¬å–å¾—"""
    rows = []
    for sid in SPREADSHEET_IDS:
        sh = open_sheet_by_id(sid)
        if not sh:
            continue

        # 1) ã‚¿ã‚¤ãƒˆãƒ«ã¨ sheetId (gid) ã‚’è»½é‡å–å¾—
        meta = None
        for attempt in range(MAX_RETRY):
            try:
                meta = sh.fetch_sheet_metadata(
                    params={"fields": "sheets(properties(title,sheetId))"}
                )
                break
            except APIError as e:
                code = getattr(getattr(e, "response", None), "status_code", None)
                if code == 429 and attempt < MAX_RETRY - 1:
                    wait = BASE_WAIT * (2 ** attempt)
                    st.warning(f"â³ Rate limit: fetch_sheet_metadata (retry {attempt+1}/{MAX_RETRY}) in {wait:.1f}s")
                    time.sleep(wait)
                    continue
                meta = None
                break
            except Exception:
                meta = None
                break
        if not meta:
            continue

        sheets_props = [s["properties"] for s in meta.get("sheets", []) if "properties" in s]
        title_to_gid = {p.get("title"): p.get("sheetId") for p in sheets_props}
        titles = [p.get("title") for p in sheets_props]

        # åˆ—å¹…ã‚’å¿…è¦ã«å¿œã˜ã¦èª¿æ•´ï¼ˆç‹­ã„ã»ã©é€Ÿã„ï¼‰
        ranges = [f"'{t}'!A:Q" for t in titles]  # ã•ã‚‰ã«è»½ãã™ã‚‹ãªã‚‰ A:N ãªã©ã¸

        # 2) ä¸€æ‹¬å–å¾—ï¼ˆvalues.batchGetï¼‰
        time.sleep(BASE_WAIT)
        vals_resp = None
        for attempt in range(MAX_RETRY):
            try:
                vals_resp = sh.values_batch_get(ranges=ranges, params={"majorDimension": "ROWS"})
                break
            except APIError as e:
                code = getattr(getattr(e, "response", None), "status_code", None)
                if code == 429 and attempt < MAX_RETRY - 1:
                    wait = BASE_WAIT * (2 ** attempt)
                    st.warning(f"â³ Rate limit: values.batchGet (retry {attempt+1}/{MAX_RETRY}) in {wait:.1f}s")
                    time.sleep(wait)
                    continue
                vals_resp = None
                break
            except Exception:
                vals_resp = None
                break
        if not vals_resp:
            continue

        for vr in vals_resp.get("valueRanges", []):
            rng = vr.get("range", "")
            ws_title = rng.split("!")[0].strip("'")
            vals = vr.get("values", [])
            if not vals:
                continue

            rec = parse_sheet(vals)
            if not any(rec.values()):
                continue

            rec["ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ"] = sh.title
            rec["ãƒ•ã‚¡ã‚¤ãƒ«ID"] = sid
            rec["ã‚·ãƒ¼ãƒˆå"] = ws_title
            rec["ã‚·ãƒ¼ãƒˆGID"] = title_to_gid.get(ws_title)
            rec["æ¤œç´¢ç”¨ãƒ†ã‚­ã‚¹ãƒˆ"] = " ".join([
                rec.get("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å",""), rec.get("ãƒ†ãƒ¼ãƒ",""), rec.get("å¯¾è±¡",""),
                rec.get("æº–å‚™ç‰©",""), rec.get("å®Ÿæ–½æ–¹æ³•",""),
                rec.get("å­ä¾›ãŸã¡ã®åå¿œ",""), rec.get("è‰¯ã‹ã£ãŸç‚¹",""), rec.get("æ”¹å–„ç‚¹","")
            ]).strip()
            rows.append(rec)

    return pd.DataFrame(rows)

# =============================================================================
# æ¤œç´¢æº–å‚™ï¼ˆåŸ‹ã‚è¾¼ã¿ï¼‹BM25ï¼‰â€” åŸ‹ã‚è¾¼ã¿ã¯é…å»¶è¨ˆç®—ï¼†ã‚­ãƒ£ãƒƒã‚·ãƒ¥
# =============================================================================
@st.cache_resource(show_spinner=True)
def load_embedder():
    return SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

@st.cache_data(show_spinner=False)
def build_bm25(corpus_tokens: List[List[str]]):
    return BM25Okapi(corpus_tokens)

def tokenize_ja(text: str) -> List[str]:
    text = normalize(text)
    toks = re.split(r"[ \u3000ã€ã€‚ãƒ»,./!?ï¼ï¼Ÿ\-\n\r\t]+", text)
    return [t for t in toks if t]

@st.cache_data(show_spinner=True, ttl=24*3600)
def embed_corpus(texts: List[str]):
    emb = load_embedder()
    return emb.encode(texts, normalize_embeddings=True, show_progress_bar=False)

SYNONYMS = {
    "ç™ºè¡¨": ["ãƒ—ãƒ¬ã‚¼ãƒ³", "ã‚¹ãƒ”ãƒ¼ãƒ", "è¡¨ç¾", "ç™ºè¡¨ç·´ç¿’"],
    "è¡¨ç¾": ["ç™ºè¡¨", "ä¼ãˆã‚‹", "ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆ"],
    "å”åŠ›": ["å”åƒ", "ãƒãƒ¼ãƒ ", "ã‚°ãƒ«ãƒ¼ãƒ—", "å…±åŒ"],
    "å‰µä½œ": ["ã‚‚ã®ã¥ãã‚Š", "åˆ¶ä½œ", "å·¥ä½œ", "ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–"],
    "èª­è§£": ["èª­ã¿å–ã‚Š", "æ„Ÿæƒ³", "èª­æ›¸", "æœ—èª­"],
}

# =============================================================================
# UI
# =============================================================================
st.title("æ´»å‹•ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¤œç´¢")

with st.sidebar:
    st.header("è¡¨ç¤ºãƒ»æ¤œç´¢è¨­å®š")
    mode = st.radio("ãƒ¢ãƒ¼ãƒ‰", ["ğŸ” æ¤œç´¢", "ğŸ“‘ ã‚·ãƒ¼ãƒˆåˆ¥ä¸€è¦§"], horizontal=False)

    # æ¤œç´¢ç”¨
    alpha = st.slider("æ„å‘³é‡è¦–ï¼ˆ1.0ï¼‰ â†â†’ èªä¸€è‡´é‡è¦–ï¼ˆ0.0ï¼‰", 0.0, 1.0, 0.7, 0.05)
    top_k = st.slider("ä»¶æ•°ï¼ˆæœ€å¤§è¨ˆç®—ä»¶æ•°ï¼‰", 50, 500, 200, step=50)

    # ã‚¹ãƒãƒ›å‘ã‘ã« 1 ã‚«ãƒ©ãƒ ã¸åˆ‡æ›¿
    is_mobile = st.toggle("ãƒ¢ãƒã‚¤ãƒ«è¡¨ç¤ºï¼ˆçµæœ1ã‚«ãƒ©ãƒ ï¼‰", value=False)

    st.caption("â€»åˆå›ã¯èª­ã¿è¾¼ã¿ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¾Œã¯é€Ÿããªã‚Šã¾ã™ï¼‰")

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
with st.spinner("ã‚·ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™â€¦"):
    df = load_all_data_v2()

# --- ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼ˆæ§ãˆã‚è¡¨ç¤ºï¼‰ï¼šãƒ­ã‚°ãŒç„¡ãã¦ã‚‚ df ã‹ã‚‰å¾©å…ƒã—ã¦å‡ºã™ ---
sources = st.session_state.get("OPENED_LOG", [])
if (not sources) and (len(df) > 0):
    tmp = (
        df[["ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ", "ãƒ•ã‚¡ã‚¤ãƒ«ID"]]
        .dropna()
        .drop_duplicates()
        .values
        .tolist()
    )
    sources = [(title, sid) for title, sid in tmp]

with st.expander(f"ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼ˆ{len(sources)}ä»¶ï¼‰", expanded=False):
    st.caption(f"ğŸ“„ èª­ã¿è¾¼ã‚ãŸãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(df)}")
    for title, sid in sources:
        if isinstance(title, str) and title.startswith("âŒ"):
            st.caption(title)
        else:
            st.caption(f"âœ… {title} ({_short_id(sid)})")

# =============================================================================
# ğŸ“‘ ã‚·ãƒ¼ãƒˆåˆ¥ä¸€è¦§ï¼ˆä¸€è¦§ãƒ¢ãƒ¼ãƒ‰ï¼‰
# =============================================================================
if mode == "ğŸ“‘ ã‚·ãƒ¼ãƒˆåˆ¥ä¸€è¦§":
    st.subheader("ã‚·ãƒ¼ãƒˆåˆ¥ä¸€è¦§")

    ss_options = (
        df[["ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ", "ãƒ•ã‚¡ã‚¤ãƒ«ID"]]
        .dropna()
        .drop_duplicates()
        .sort_values("ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ")
        .values
        .tolist()
    )
    ss_names = ["ï¼ˆã™ã¹ã¦ï¼‰"] + [name for name, _id in ss_options]
    selected_ss = st.selectbox("ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’é¸æŠ", ss_names, index=0)

    if selected_ss == "ï¼ˆã™ã¹ã¦ï¼‰":
        df_view = df
    else:
        df_view = df[df["ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ"] == selected_ss]

    grp = (
        df_view.groupby(["ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ", "ãƒ•ã‚¡ã‚¤ãƒ«ID", "ã‚·ãƒ¼ãƒˆå", "ã‚·ãƒ¼ãƒˆGID"])
              .size().reset_index(name="ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°")
              .sort_values(["ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ", "ã‚·ãƒ¼ãƒˆå"])
    )

    if grp.empty:
        st.info("è©²å½“ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        st.stop()

    for (ss_name, file_id), chunk in grp.groupby(["ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ", "ãƒ•ã‚¡ã‚¤ãƒ«ID"]):
        with st.expander(f"{ss_name}ï¼ˆ{len(chunk)}ã‚·ãƒ¼ãƒˆï¼‰", expanded=False):
            for _, r in chunk.iterrows():
                gid = r["ã‚·ãƒ¼ãƒˆGID"]
                if pd.notna(gid):
                    url = f"https://docs.google.com/spreadsheets/d/{file_id}/edit#gid={int(gid)}"
                else:
                    url = f"https://docs.google.com/spreadsheets/d/{file_id}/edit"

                # ãƒ†ãƒ¼ãƒã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆé‡è¤‡é™¤å¤–ãƒ»æœ€å¤§3ä»¶ï¼‰
                themes_series = (
                    df[(df["ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ"] == ss_name) & (df["ã‚·ãƒ¼ãƒˆå"] == r["ã‚·ãƒ¼ãƒˆå"])]
                    ["ãƒ†ãƒ¼ãƒ"].fillna("").map(normalize)
                )
                themes_uniq, seen = [], set()
                for t in themes_series:
                    if t and t not in seen:
                        seen.add(t)
                        themes_uniq.append(t)
                    if len(themes_uniq) >= 3:
                        break

                st.markdown(f"- [{r['ã‚·ãƒ¼ãƒˆå']}]({url})")
                if themes_uniq:
                    st.caption(" ï¼ ".join(themes_uniq))

    st.stop()  # ä¸€è¦§ãƒ¢ãƒ¼ãƒ‰çµ‚äº†

# =============================================================================
# ğŸ” æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰
# =============================================================================
# æ¤œç´¢ã‚³ãƒ¼ãƒ‘ã‚¹ã®æº–å‚™ï¼ˆBM25 ã¯å…ˆã«æ§‹ç¯‰ / åŸ‹ã‚è¾¼ã¿ã¯é…å»¶ï¼‰
corpus_texts = (df["æ¤œç´¢ç”¨ãƒ†ã‚­ã‚¹ãƒˆ"].fillna("") + " " + df["ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å"].fillna("")).tolist()
corpus_tokens = [tokenize_ja(t) for t in corpus_texts]
bm25 = build_bm25(corpus_tokens)

st.caption("ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦æ¤œç´¢")
q = st.text_input(
    label="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›",
    value="",
    placeholder="ä¾‹: ç™ºè¡¨ç·´ç¿’, ã‚°ãƒ«ãƒ¼ãƒ—æ´»å‹•, æœ—èª­, å·¥ä½œ, è¡¨ç¾åŠ› ãªã©",
    label_visibility="collapsed",
)

# æ¤œç´¢èªãŒå¤‰ã‚ã£ãŸã‚‰è¡¨ç¤ºä»¶æ•°ã‚’ãƒªã‚»ãƒƒãƒˆ
if q != st.session_state.last_query:
    st.session_state.show_n = 15
    st.session_state.last_query = q

if q:
    expanded = [q]
    for k, vs in SYNONYMS.items():
        if k in q:
            expanded += vs
    q_expanded = " ".join(expanded)

    # BM25
    q_toks = tokenize_ja(q_expanded)
    bm25_scores = bm25.get_scores(q_toks)

    # â˜… åŸ‹ã‚è¾¼ã¿ã¯ã“ã“ã§åˆã‚ã¦å®Ÿè¡Œï¼ˆï¼‹24h ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
    corpus_emb = embed_corpus(corpus_texts)
    q_emb = load_embedder().encode([q_expanded], normalize_embeddings=True, show_progress_bar=False)
    sem_scores = util.cos_sim(q_emb, corpus_emb).cpu().numpy()[0]

    # ã‚¹ã‚³ã‚¢æ­£è¦åŒ–
    def minmax(x):
        x = np.array(x, dtype=float)
        if x.max() - x.min() < 1e-9:
            return np.zeros_like(x)
        return (x - x.min()) / (x.max() - x.min())

    bm25_n = minmax(bm25_scores)
    sem_n  = minmax(sem_scores)
    final  = alpha * sem_n + (1 - alpha) * bm25_n

    # ä¸Šä½ top_k ã¾ã§å–å¾—ï¼ˆæ®µéšè¡¨ç¤ºã§ä¼¸ã°ã›ã‚‹ä¸Šé™ï¼‰
    idx_all = np.argsort(final)[::-1][:top_k]

    # ã„ã¾è¡¨ç¤ºã™ã‚‹ä»¶æ•°ï¼ˆ15ä»¶ãšã¤å¢—ãˆã‚‹ï¼‰
    show_n = min(st.session_state.show_n, len(idx_all))
    idx = idx_all[:show_n]

    st.subheader(f"æ¤œç´¢çµæœï¼ˆ{len(idx_all)}ä»¶ä¸­ {show_n}ä»¶ã‚’è¡¨ç¤ºï¼‰")

    for rank, i in enumerate(idx, start=1):
        row = df.iloc[i]
        gid = row.get("ã‚·ãƒ¼ãƒˆGID")
        fid = row["ãƒ•ã‚¡ã‚¤ãƒ«ID"]
        if pd.notna(gid):
            url = f"https://docs.google.com/spreadsheets/d/{fid}/edit#gid={int(gid)}"
        else:
            url = f"https://docs.google.com/spreadsheets/d/{fid}/edit"

        cols = st.columns(1 if is_mobile else 3)

        with st.container(border=True):
            st.markdown(
                f"**{rank}. {row.get('ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å','(åç§°æœªè¨­å®š)')}** ã€€"
                f"[{row['ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ']} / {row['ã‚·ãƒ¼ãƒˆå']}]({url})"
            )
            with cols[0]:
                st.write("**ãƒ†ãƒ¼ãƒ**:", row.get("ãƒ†ãƒ¼ãƒ",""))
                st.write("**å¯¾è±¡**:", row.get("å¯¾è±¡",""))
                st.write("**å‚åŠ äººæ•°**:", row.get("å‚åŠ äººæ•°",""))
            if not is_mobile:
                with cols[1]:
                    st.write("**æº–å‚™ç‰©**:", row.get("æº–å‚™ç‰©",""))
                    st.write("**å®Ÿæ–½æ–¹æ³•**:", row.get("å®Ÿæ–½æ–¹æ³•",""))
                with cols[2]:
                    st.write("**å­ä¾›ãŸã¡ã®åå¿œ**:", row.get("å­ä¾›ãŸã¡ã®åå¿œ",""))
                    st.write("**è‰¯ã‹ã£ãŸç‚¹**:", row.get("è‰¯ã‹ã£ãŸç‚¹",""))
                    st.write("**æ”¹å–„ç‚¹**:", row.get("æ”¹å–„ç‚¹",""))
            st.caption(f"score={final[i]:.3f} / semantic={sem_n[i]:.3f} / bm25={bm25_n[i]:.3f}")

    # ã•ã‚‰ã«è¡¨ç¤º
    if show_n < len(idx_all):
        c1, c2, _ = st.columns([1, 1, 6])
        if c1.button("ã•ã‚‰ã«è¡¨ç¤ºï¼ˆ+15ï¼‰"):
            st.session_state.show_n = min(show_n + 15, len(idx_all))
            st.rerun()
        if c2.button("å…¨ä»¶è¡¨ç¤º"):
            st.session_state.show_n = len(idx_all)
            st.rerun()
